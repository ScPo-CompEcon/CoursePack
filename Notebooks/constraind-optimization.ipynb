{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Computational Economics: Constrained Optimization\n",
    "% Florian Oswald\n",
    "% Sciences Po, 2016\n",
    "\n",
    "----------------\n",
    "\n",
    "# Constrained Optimisation\n",
    "\n",
    "* Recall our generic definition of an optimization problem:\n",
    "\t$$ \\min_{x\\in\\mathbb{R}^n} f(x)  \\quad  s.t.\\quad \\begin{array} c_i(x) = 0, & i\\in E \\\\\n",
    "                                                              c_i(x) \\geq 0, & i\\in I \\end{array}\n",
    "                                                              $$\n",
    "* E is the set of *equality constraints* and I is the set of *inequality constraints*.\n",
    "* **Defintion: The Feasible Set**: Let $\\Omega$ be the set of points $x$ that satisfy the constraints, i.e.\n",
    "\t$$ \\Omega = {x|c_i(x)=0,i\\in E; c_i(x)\\geq 0, i\\in I} $$\n",
    "* Then, a different way of writign our problem is\n",
    "\t$$ \\min_{x\\in \\Omega} f(x)  $$\n",
    "* A vector $x^*$ is a *local solution* to this problem if $x^* \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ s.t. $f(x)\\geq f(x^*),\\forall x\\in \\mathcal{N} \\cap \\Omega$.\n",
    "* **Definition: The Active Set**: Active set $\\mathcal{A}(x)$ at any feasible $x$ consists of the equality constraint indices from E together with the indices of the inequality constraints for $i$ for which $c_i(x) = 0$; that is,\n",
    "\t$$ \\matcal{A}(x) = E \\cup {i\\in I|c_i(x) = 0} $$\n",
    "\tAt a feasible point $x$, the inequality constraint $i \\in I$ is said to be *active* if $c_i(x)=0$, and *inactive* if $c_i(x)>0$\n",
    "\n",
    "## Nonlinear Constraints\n",
    "\n",
    "* Consider the following problem\n",
    "\t$$ \\min_{x \\in \\mathbb{R}^2} \\sqrt{x_2}\\quad s.t. \\quad \\begin{array}{c}        \\\\\n",
    "\t                                                                                x_2 \\geq 0 \\\\\n",
    "\t                                                                         x_2 \\geq (a_1 x_1 + b_1)^3 \\\\\n",
    "\t                                                                         x_2 \\geq (a_2 x_1 + b_2)^3 \\end{array}\n",
    "\t$\n",
    "* This configuration of constraints leads to the following feasible region for parameters $a_1=2,b_1=0,a_2=-1,b_2=1$.\n",
    "<img src=\"figs/NLopt-example-constraints.png\" width=\"800\" height=\"600\" />\n",
    "\n",
    "----------------\n",
    "\n",
    "# Example: 1 equality constraint\n",
    "\n",
    "* consider \n",
    "\t$$ \\min x_1 + x_2  \\quad s.t. \\quad x_1^2 + x_2^2 - 2 = 0 $$\n",
    "* constraint is a circle with radius $\\sqrt{2}$ centered at 0. The solution must lie *on* that circle.\n",
    "* Solution: $(-1,-1)$. Consider any other point on circle, like $(\\sqrt{2},0)$.\n",
    "\n",
    "![Figure 12.3 in [@nocedal-wright]](figs-restricted/constrained-example.png) \n",
    "\n",
    "----------------\n",
    "\n",
    "# Example: 1 inequality constraint\n",
    "\n",
    "* Let's modify this example to\n",
    "\t$$ \\min x_1 + x_2  \\quad s.t. \\quad 2- x_1^2 - x_2^2 \\geq 0 $$\n",
    "* constraint is the region inside a circle with radius $\\sqrt{2}$ centered at 0. The solution must lie *on or inside* that circle.\n",
    "* Solution: $(-1,-1)$. Consider any other point on circle, like $(\\sqrt{2},0)$.\n",
    "* Two cases: \n",
    "\t1. $x$ lies strictly inside the circle, and $c_1(x) > 0$\n",
    "\t1. $x$ lies strictly on the circle, and $c_1(x) = 0$\n",
    "* Complementarity condition.\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# First Order Optimality Conditions\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# Some Methods \n",
    "\n",
    "* Penalty Function and Augmented Lagrangian Methods\n",
    "* Sequential Quadratic Method\n",
    "* Interior Point Method\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# Penalty Function and Augmented Lagrangian Methods\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# First Order Optimality Conditions\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "# Constrained Optimisation with [`NLopt.jl`](https://github.com/JuliaOpt/NLopt.jl)\n",
    "\n",
    "* We need to specify one function for each objective and constraint.\n",
    "* Both of those functions need to compute the function value (i.e. objective or constraint) *and* it's respective gradient. \n",
    "* Notice that we can disregard $x_2\\geq0$ here.\n",
    "* `NLopt` expects contraints **always** to be formulated in the format \n",
    "\t$$\n",
    "\t\\texttt{constraint_function}(x) \\leq 0 $$\n",
    "* The constraint function is formulated for each constraint at $x$. it returns a number (the value of the constraint at $x$), and it fills out the gradient vector, which is the partial derivative of the current constraint wrt $x$.\n",
    "* There is also the option to have vector valued constraints, see the documentation.\n",
    "* We set this up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "function myfunc(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 0\n",
    "        grad[2] = 0.5/sqrt(x[2])\n",
    "    end\n",
    "    return sqrt(x[2])\n",
    "end\n",
    "\n",
    "function constraint(x::Vector, grad::Vector, a, b)\n",
    "    if length(grad) > 0\n",
    "    \t# modifies grad in place\n",
    "        grad[1] = 3a * (a*x[1] + b)^2\n",
    "        grad[2] = -1\n",
    "    end\n",
    "    return (a*x[1] + b)^3 - x[2]\n",
    "end\n",
    "using NLopt\n",
    "# define an Opt object: which algorithm, how many dims of choice\n",
    "opt = Opt(:LD_MMA, 2)\n",
    "# set bounds and tolerance\n",
    "lower_bounds!(opt, [-Inf, 0.])\n",
    "xtol_rel!(opt,1e-4)\n",
    "\n",
    "# define objective function\n",
    "min_objective!(opt, myfunc)\n",
    "# define constraints\n",
    "# notice the anonymous function\n",
    "inequality_constraint!(opt, (x,g) -> constraint(x,g,2,0), 1e-8)\n",
    "inequality_constraint!(opt, (x,g) -> constraint(x,g,-1,1), 1e-8)\n",
    "\n",
    "# call optimize\n",
    "(minf,minx,ret) = optimize(opt, [1.234, 5.678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# NLopt: Rosenbrock\n",
    "\n",
    "* Let's tackle the rosenbrock example.\n",
    "* To make it more interesting, let's add an inequality constraint.\n",
    "\t$$ \\min_{x\\in \\mathbb{R}^2} (1-x_1)^2  + 100(x_2-x_1^2)^2  \\quad s.t. \\quad 0.8 - x_1^2 -x_2^2 \\geq 0 $$\n",
    "* in `NLopt` format, the constraint is $x_1 + x_2 - 0.8 \\leq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "function rosenbrock(x::Vector,grad::Vector)\n",
    "    if length(grad) > 0\n",
    "\t    grad[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "\t    grad[2] = 200.0 * (x[2] - x[1]^2)\n",
    "    end\n",
    "    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end\n",
    "function r_constraint(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "\tgrad[1] = 2*x[1]\n",
    "\tgrad[2] = 2*x[2]\n",
    "\tend\n",
    "\treturn x[1]^2 + x[2]^2 - 0.8\n",
    "end\n",
    "opt = Opt(:LD_MMA, 2)\n",
    "lower_bounds!(opt, [-5, -5.0])\n",
    "min_objective!(opt,(x,g) -> rosenbrock(x,g))\n",
    "inequality_constraint!(opt, (x,g) -> r_constraint(x,g))\n",
    "ftol_rel!(opt,1e-9)\n",
    "(minf,minx,ret) = optimize(opt, [-1.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "# JuMP\n",
    "\n",
    "* Introduce [`JuMP.jl`](https://github.com/JuliaOpt/JuMP.jl)\n",
    "* JuMP is a mathematical programming interface for Julia. It is like AMPL, but for free and with a decent programming language.\n",
    "* The main highlights are:\n",
    "\t* It uses automatic differentiation to compute derivatives from your expression.\n",
    "\t* It supplies this information, as well as the sparsity structure of the Hessian to your preferred solver.\n",
    "\t* It decouples your problem completely from the type of solver you are using. This is great, since you don't have to worry about different solvers having different interfaces.\n",
    "\t* In order to achieve this, `JuMP` uses [`MathProgBase.jl`](https://github.com/JuliaOpt/MathProgBase.jl), which converts your problem formulation into a standard representation of an optimization problem.\n",
    "* Let's look at the readme.\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "# JuMP: Example\n",
    "\n",
    "* Instead of hand-coding first and second derivatives, you only have to give `JuMP` expressions for objective and constraints.\n",
    "* Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "using JuMP\n",
    "\n",
    "let\n",
    "\n",
    "    m = Model()\n",
    "\n",
    "    @defVar(m, x)\n",
    "    @defVar(m, y)\n",
    "\n",
    "    @setNLObjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "\n",
    "    solve(m)\n",
    "\n",
    "    println(\"x = \", getValue(x), \" y = \", getValue(y))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* not bad, right?\n",
    "* adding the constraint from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    m = Model()\n",
    "\n",
    "    @defVar(m, x)\n",
    "    @defVar(m, y)\n",
    "\n",
    "    @setNLObjective(m, Min, (1-x)^2 + 100(y-x^2)^2)\n",
    "    @addNLConstraint(m,x^2 + y^2 <= 0.8)\n",
    "\n",
    "    solve(m)\n",
    "\n",
    "    println(\"x = \", getValue(x), \" y = \", getValue(y))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Gamma\t = 0\n",
    "\n",
    "# JuMP: Maximium Likelihood\n",
    "\n",
    "* Let's redo the maximum likelihood example in JuMP.\n",
    "* Let $\\mu,\\sigma^2$ be the unknown mean and variance of a random sample generated from the normal distribution.\n",
    "* Find the maximum likelihood estimator for those parameters!\n",
    "* density:\n",
    "\t$$ f(x_i|\\mu,\\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) $$\n",
    "* Likelihood Function\n",
    "\t$$ \\begin{align} L(\\mu,\\sigma^2) = \\Pi_{i=1}^N f(x_i|\\mu,\\sigma^2) =& \\frac{1}{(\\sigma \\sqrt{2\\pi})^n} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 \\right) \\\\\n",
    "\t =& \\left(\\sigma^2 2\\pi\\right)^{-\\frac{n}{2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 \\right) \\end{align} $$\n",
    "* Constraints: $\\mu\\in \\mathbb{R},\\sigma>0$\n",
    "* log-likelihood: \n",
    "\t$$ \\log L = l = -\\frac{n}{2} \\log \\left( 2\\pi \\sigma^2 \\right) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2 $$\n",
    "* Let's do this in `JuMP`.\n",
    "\n",
    ". . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Copyright 2015, Iain Dunning, Joey Huchette, Miles Lubin, and contributors\n",
    "#  example modified \n",
    "using JuMP\n",
    "using Distributions\n",
    "\n",
    "distrib = Normal(4.5,3.5)\n",
    "n = 10000\n",
    "\n",
    "data = rand(distrib,n);\n",
    "\n",
    "m = Model()\n",
    "\n",
    "@defVar(m, mu, start = 0.0)\n",
    "@defVar(m, sigma >= 0.0, start = 1.0)\n",
    "\n",
    "@setNLObjective(m, Max, -(n/2)*log(2π*sigma^2)-sum{(data[i]-mu)^2, i=1:n}/(2*sigma^2))\n",
    "\n",
    "solve(m)\n",
    "println(\"μ = \", getValue(mu),\", mean(data) = \", mean(data))\n",
    "println(\"σ^2 = \", getValue(sigma)^2, \", var(data) = \", var(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "# References"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
